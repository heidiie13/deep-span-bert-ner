{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from models.encoders.bert_like import BertLikeConfig\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        'tokens': ['John', 'Smith', 'works', 'at', 'HelloGoogle', '.'],\n",
    "        'chunks': [('PER', 0, 2), ('ORG', 4, 5)],\n",
    "    },\n",
    "    {\n",
    "        'tokens': ['Jane', 'Doe', 'is', 'a', 'doctor', '.', \"xin chao\",\"toi ten\"],\n",
    "        'chunks': [('PER', 0, 2)],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in data[0]['tokens']:\n",
    "    out=tokenizer.encode(token)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_tok_ids': tensor([ 101, 2198, 3044, 2573, 2012, 7592, 3995, 8649, 2571, 1012,  102]),\n",
       " 'ori_indexes': tensor([0, 1, 2, 3, 4, 4, 4, 4, 5])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = config.exemplify(data[0])\n",
    "example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] john smith works at hellogoogle. [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([ 101, 2198, 3044, 2573, 2012, 7592, 3995, 8649, 2571, 1012,  102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_tok_ids': tensor([  101,  4869, 18629,  2003,  1037,  3460,  1012,  8418,  2078, 22455,\n",
       "          2000,  2072,  2702,   102]),\n",
       " 'ori_indexes': tensor([0, 1, 2, 3, 4, 5, 6, 6, 6, 7, 7, 7])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example2 = config.exemplify(data[1])\n",
    "example2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sub_tok_ids': tensor([[  101,  2198,  3044,  2573,  2012,  7592,  3995,  8649,  2571,  1012,\n",
       "            102,     0,     0,     0],\n",
       "         [  101,  4869, 18629,  2003,  1037,  3460,  1012,  8418,  2078, 22455,\n",
       "           2000,  2072,  2702,   102]]),\n",
       " 'sub_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True]]),\n",
       " 'ori_indexes': tensor([[ 0,  1,  2,  3,  4,  4,  4,  4,  5, -1, -1, -1],\n",
       "         [ 0,  1,  2,  3,  4,  5,  6,  6,  6,  7,  7,  7]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = config.batchify([example1, example2])\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertLikeConfig(tokenizer=tokenizer, bert_like=bert_model, output_hidden_states=True)\n",
    "embedder = config.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch['sub_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 768])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_hidden, all_bert_hidden, sub_mask = embedder(**batch)\n",
    "all_bert_hidden[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.encoders.query_bert_like' from 'e:\\\\KLTN\\\\DSpERT2\\\\models\\\\encoders\\\\query_bert_like.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from models.encoders import bert_like, query_bert_like\n",
    "from models.encoders import span_bert_like\n",
    "\n",
    "importlib.reload(span_bert_like)\n",
    "\n",
    "importlib.reload(bert_like)\n",
    "importlib.reload(query_bert_like)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_bert_like.QueryBertLikeEncoder(bert_model.encoder, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1,2,3,4,5]\n",
    "test[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "span_config = span_bert_like.SpanBertLikeConfig(\n",
    "        bert_like=bert_model,\n",
    "        max_span_size=4,\n",
    "        init_agg_mode='scaled_dot_attention',\n",
    "        init_drop_rate=0.2,\n",
    "        share_weights_ext=True,\n",
    "        share_weights_int=True,\n",
    "        num_layers=2\n",
    "    )\n",
    "span_encoder = span_config.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 768])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_outputs = span_encoder(all_bert_hidden)\n",
    "span_outputs[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bert_hidden[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch['ori_indexes'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import importlib\n",
    "from models.encoders import encode\n",
    "importlib.reload(encode)\n",
    "from models.encoders.encode import EncoderConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 768])\n",
      "torch.Size([2, 10, 128])\n",
      "torch.Size([2, 10, 256])\n",
      "torch.Size([2, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# Identity Encoder\n",
    "config = EncoderConfig(arch='IDENTITY', in_dim=768, in_drop_rates=0.1, hid_drop_rate=0.0)\n",
    "identity_encoder = config.instantiate()\n",
    "embedded = torch.randn(2, 10, 768)\n",
    "output = identity_encoder(embedded)\n",
    "print(output.shape)  # [2, 10, 768]\n",
    "\n",
    "# FFN Encoder\n",
    "config = EncoderConfig(arch='FFN', in_dim=768, hid_dim=128, num_layers=2, in_drop_rates=0.1, hid_drop_rate=0.5)\n",
    "ffn_encoder = config.instantiate()\n",
    "output = ffn_encoder(embedded)\n",
    "print(output.shape)  # [2, 10, 128]\n",
    "\n",
    "# RNN Encoder (LSTM)\n",
    "config = EncoderConfig(arch='LSTM', in_dim=768, hid_dim=256, num_layers=1, in_drop_rates=0.1, hid_drop_rate=0.5)\n",
    "rnn_encoder = config.instantiate()\n",
    "output = rnn_encoder(embedded)\n",
    "print(output.shape)  # [2, 10, 256]\n",
    "\n",
    "# RNN Encoder (GRU)\n",
    "config = EncoderConfig(arch='GRU', in_dim=768, hid_dim=256, num_layers=2, in_drop_rates=0.0, hid_drop_rate=0.5)\n",
    "gru_encoder = config.instantiate()\n",
    "output = gru_encoder(embedded)\n",
    "print(output.shape)  # [2, 10, 256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import seq_lens2mask\n",
    "import torch\n",
    "seq_lens2mask(torch.tensor([1,2,3]),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3]]).squeeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5173, -1.3610,  0.7019],\n",
       "        [-1.1862,  2.3196,  0.8788]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1596,  0.3546,  1.0000],\n",
       "        [-1.1154, -1.8652, -1.4587],\n",
       "        [-0.6398, -2.4668,  0.0357]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randn(3, 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5173, -1.3610,  0.7019],\n",
       "        [-1.1862,  2.3196,  0.8788],\n",
       "        [ 1.1596,  0.3546,  1.0000],\n",
       "        [-1.1154, -1.8652, -1.4587],\n",
       "        [-0.6398, -2.4668,  0.0357]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x,y], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = torch.nn.Embedding(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2587, -0.6512,  1.3804],\n",
       "        [ 0.2587, -0.6512,  1.3804]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedd(torch.tensor([1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['helo', 'hi']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['helo'] + [\"hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpecificSpanExtractor(\n",
       "  (bert_like): BertLikeEmbedder(\n",
       "    (bert_like): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (group_agg): SequenceGroupAggregating()\n",
       "  )\n",
       "  (span_bert_like): SpanBertLikeEncoder(\n",
       "    (init_aggregating): SequencePooling()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (query_bert_like): QueryBertLikeEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x QueryBertLikeLayer(\n",
       "          (attention): QueryBertLikeAttention(\n",
       "            (self): QueryBertLikeSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (intermediate2): RNNEncoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (rnn): LSTM(768, 200, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (decoder): SpecificSpanClsDecoder(\n",
       "    (affine): FFNEncoder(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (ff_blocks): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=425, out_features=300, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (size_embedding): Embedding(6, 25)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (hid2logit): Linear(in_features=300, out_features=3, bias=True)\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 111,165,501\n",
      "Trainable parameters: 111,165,501\n",
      "cuda\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 192\u001b[0m\n\u001b[0;32m    189\u001b[0m     inference(model, dataloader, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 181\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Huấn luyện\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Đánh giá\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     71\u001b[0m     obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Tính loss\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Lan truyền ngược và cập nhật tham số\u001b[39;00m\n",
      "File \u001b[1;32me:\\KLTN\\DSpERT2\\models\\specific_span_extractor.py:96\u001b[0m, in \u001b[0;36mSpecificSpanExtractor.forward\u001b[1;34m(self, batch, return_states)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Dict, return_states: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Dict, \u001b[38;5;28mtuple\u001b[39m]:\n\u001b[1;32m---> 96\u001b[0m     states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward2states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(batch, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstates)\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_states:\n",
      "File \u001b[1;32me:\\KLTN\\DSpERT2\\models\\specific_span_extractor.py:77\u001b[0m, in \u001b[0;36mSpecificSpanExtractor.forward2states\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     73\u001b[0m all_last_query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_bert_like(all_bert_hidden)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# Xử lý bert_hidden\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     bert_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# Xử lý all_last_query_states\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     new_all_last_query_states \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32me:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32me:\\KLTN\\DSpERT2\\models\\encoders\\encode.py:51\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, embedded, mask)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedded: torch\u001b[38;5;241m.\u001b[39mTensor, mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chuyển từ embedded sang hidden states với mask tùy chọn.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedded2hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden\n",
      "File \u001b[1;32me:\\KLTN\\DSpERT2\\models\\encoders\\encode.py:109\u001b[0m, in \u001b[0;36mRNNEncoder.embedded2hidden\u001b[1;34m(self, embedded, mask)\u001b[0m\n\u001b[0;32m    105\u001b[0m lengths \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m    106\u001b[0m lengths \u001b[38;5;241m=\u001b[39m lengths\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39membedded\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    107\u001b[0m packed \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\n\u001b[0;32m    108\u001b[0m     embedded,\n\u001b[1;32m--> 109\u001b[0m     \u001b[43mlengths\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    110\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    111\u001b[0m     enforce_sorted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    113\u001b[0m rnn_outs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(packed)\n\u001b[0;32m    114\u001b[0m rnn_outs, _ \u001b[38;5;241m=\u001b[39m pad_packed_sequence(rnn_outs, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, total_length\u001b[38;5;241m=\u001b[39membedded\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='example.log', filemode='a', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from typing import List, Dict\n",
    "import torch.optim as optim\n",
    "\n",
    "from models.specific_span_extractor import SpecificSpanExtractorConfig, SpecificSpanExtractor\n",
    "from models.decoders.specific_span_classification import SpecificSpanClsDecoderConfig\n",
    "from models.encoders.encode import EncoderConfig\n",
    "from utils import seq_lens2mask\n",
    "from models.encoders.bert_like import BertLikeConfig\n",
    "from models.encoders.span_bert_like import SpanBertLikeConfig\n",
    "\n",
    "# data = [\n",
    "#     {'tokens': ['John', 'Smith', 'works', 'at', 'HelloGoogle', '.'], 'chunks': [('PER', 0, 2), ('ORG', 4, 5)]},\n",
    "#     {'tokens': ['Jane', 'Doe', 'is', 'a', 'doctor', '.', 'xin chao', 'toi ten'], 'chunks': [('PER', 0, 2)]}\n",
    "# ]\n",
    "from utils import read_json\n",
    "test_data = read_json(\"data/processed_data/phoner_covid19/test.json\")\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, device='cpu', num_epochs=5):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Chuyển batch sang device\n",
    "            batch['bert_like']['sub_tok_ids'] = batch['bert_like']['sub_tok_ids'].to(device)\n",
    "            batch['bert_like']['sub_mask'] = batch['bert_like']['sub_mask'].to(device)\n",
    "            batch['bert_like']['ori_indexes'] = batch['bert_like']['ori_indexes'].to(device)\n",
    "            batch['seq_lens'] = batch['seq_lens'].to(device)\n",
    "            batch['mask'] = batch['mask'].to(device)\n",
    "            for obj in batch['boundaries_objs']:\n",
    "                obj['label_ids'] = obj['label_ids'].to(device)\n",
    "            \n",
    "            # Tính loss\n",
    "            losses = model.forward(batch)\n",
    "            loss = losses.mean()\n",
    "            \n",
    "            # Lan truyền ngược và cập nhật tham số\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "def evaluate(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_chunks = []\n",
    "    all_gold_chunks = [entry['chunks'] for entry in data]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Chuyển batch sang device\n",
    "            batch['bert_like']['sub_tok_ids'] = batch['bert_like']['sub_tok_ids'].to(device)\n",
    "            batch['bert_like']['sub_mask'] = batch['bert_like']['sub_mask'].to(device)\n",
    "            batch['bert_like']['ori_indexes'] = batch['bert_like']['ori_indexes'].to(device)\n",
    "            batch['seq_lens'] = batch['seq_lens'].to(device)\n",
    "            batch['mask'] = batch['mask'].to(device)\n",
    "            for obj in batch['boundaries_objs']:\n",
    "                obj['label_ids'] = obj['label_ids'].to(device)\n",
    "            \n",
    "            # Tính loss\n",
    "            losses = model.forward(batch)\n",
    "            total_loss += losses.mean().item()\n",
    "            \n",
    "            # Dự đoán chunks\n",
    "            chunks = model.decode(batch)\n",
    "            all_chunks.extend(chunks)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
    "    print(\"Predicted chunks:\", all_chunks)\n",
    "    print(\"Gold chunks:\", all_gold_chunks)\n",
    "    \n",
    "    # Tính precision, recall, F1 (micro-average đơn giản)\n",
    "    n_gold = sum(len(chunks) for chunks in all_gold_chunks)\n",
    "    n_pred = sum(len(chunks) for chunks in all_chunks)\n",
    "    n_true_positive = sum(len(set(g) & set(p)) for g, p in zip(all_gold_chunks, all_chunks))\n",
    "    precision = n_true_positive / n_pred if n_pred > 0 else 0\n",
    "    recall = n_true_positive / n_gold if n_gold > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    return avg_loss, precision, recall, f1\n",
    "\n",
    "def inference(model, dataloader, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_chunks = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Chuyển batch sang device\n",
    "            batch['bert_like']['sub_tok_ids'] = batch['bert_like']['sub_tok_ids'].to(device)\n",
    "            batch['bert_like']['sub_mask'] = batch['bert_like']['sub_mask'].to(device)\n",
    "            batch['bert_like']['ori_indexes'] = batch['bert_like']['ori_indexes'].to(device)\n",
    "            batch['seq_lens'] = batch['seq_lens'].to(device)\n",
    "            batch['mask'] = batch['mask'].to(device)\n",
    "            for obj in batch['boundaries_objs']:\n",
    "                obj['label_ids'] = obj['label_ids'].to(device)\n",
    "            \n",
    "            # Dự đoán chunks\n",
    "            chunks = model.decode(batch)\n",
    "            all_chunks.extend(chunks)\n",
    "    \n",
    "    print(\"Inference chunks:\", all_chunks)\n",
    "    return all_chunks\n",
    "\n",
    "def main():\n",
    "    # Thiết lập mô hình và dataloader\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "    bert_like_config = BertLikeConfig(tokenizer=tokenizer, bert_like=bert_model, freeze=False)\n",
    "    span_bert_like_config = SpanBertLikeConfig(bert_like=bert_model, freeze=False, share_weights_ext=True, share_weights_int=True)\n",
    "    decoder_config = SpecificSpanClsDecoderConfig(max_span_size=5)\n",
    "    extractor_config = SpecificSpanExtractorConfig(\n",
    "        decoder=decoder_config,\n",
    "        bert_like=bert_like_config,\n",
    "        span_bert_like=span_bert_like_config\n",
    "    )\n",
    "\n",
    "    dataset = Dataset(test_data, extractor_config, training=True)\n",
    "    dataset.build_vocabs_and_dims(test_data)\n",
    "    dataloader = DataLoader(dataset, batch_size=2, collate_fn=dataset.collate)\n",
    "\n",
    "    model = extractor_config.instantiate()\n",
    "\n",
    "    # Đếm tham số\n",
    "    count_parameters(model)\n",
    "\n",
    "    # Thiết lập optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "    # Huấn luyện\n",
    "    print(\"\\nTraining...\")\n",
    "    train(model, dataloader, optimizer, device=device, num_epochs=5)\n",
    "\n",
    "    # Đánh giá\n",
    "    print(\"\\nEvaluating...\")\n",
    "    evaluate(model, dataloader, device=device)\n",
    "\n",
    "    # Suy luận\n",
    "    print(\"\\nInference...\")\n",
    "    inference(model, dataloader, device=device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataset import Dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "\n",
    "bert_like_config = BertLikeConfig(tokenizer=tokenizer, bert_like=bert_model, freeze=False)\n",
    "span_bert_like_config = SpanBertLikeConfig(bert_like=bert_model, max_span_size=5, freeze=False, share_weights_ext=True, share_weights_int=True)\n",
    "decoder_config = SpecificSpanClsDecoderConfig(idx2label=['PER', 'ORG'], max_span_size=5)\n",
    "extractor_config = SpecificSpanExtractorConfig(\n",
    "    decoder=decoder_config,\n",
    "    bert_like=bert_like_config,\n",
    "    span_bert_like=span_bert_like_config\n",
    ")\n",
    "data = [\n",
    "    {'tokens': ['John', 'Smith', 'works', 'at', 'HelloGoogle', '.'], 'chunks': [('PER', 0, 2), ('ORG', 4, 5)]},\n",
    "    {'tokens': ['Jane', 'Doe', 'is', 'a', 'doctor', '.', 'xin chao', 'toi ten'], 'chunks': [('PER', 0, 2)]}\n",
    "]\n",
    "dataset = Dataset(test_data, extractor_config, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset consists 3,000 sequences\n",
      "The average `tokens` length is 36.1\n",
      "The maximum `tokens` length is 185\n",
      "The dataset has 11,735 chunks of 10 types\n"
     ]
    }
   ],
   "source": [
    "print(dataset.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_json\n",
    "test_data = read_json(\"data/processed_data/phoner_covid19/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Path (sys.path):\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\\python311.zip\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\\DLLs\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\\Lib\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\n",
      "\n",
      "C:\\Users\\ADMIN\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\\win32\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\\win32\\lib\n",
      "e:\\miniconda\\miniconda3\\envs\\dspert\\Lib\\site-packages\\Pythonwin\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "def check_pythonpath():\n",
    "    \"\"\"\n",
    "    In ra tất cả các đường dẫn trong sys.path để kiểm tra Python Path.\n",
    "    \"\"\"\n",
    "    print(\"Python Path (sys.path):\")\n",
    "    for path in sys.path:\n",
    "        print(path)\n",
    "\n",
    "check_pythonpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bác',\n",
       " 'sĩ',\n",
       " 'Trần',\n",
       " 'Thanh',\n",
       " 'Linh',\n",
       " ',',\n",
       " 'từ',\n",
       " 'Bệnh',\n",
       " 'viện',\n",
       " 'Chợ',\n",
       " 'Rẫy',\n",
       " 'chi',\n",
       " 'viện',\n",
       " 'phụ',\n",
       " 'trách',\n",
       " 'đơn',\n",
       " 'nguyên',\n",
       " 'hồi',\n",
       " 'sức',\n",
       " 'tích',\n",
       " 'cực',\n",
       " ',',\n",
       " 'cho',\n",
       " 'biết',\n",
       " '\"',\n",
       " 'bệnh',\n",
       " 'nhân',\n",
       " '416',\n",
       " '\"',\n",
       " 'vẫn',\n",
       " 'đang',\n",
       " 'duy',\n",
       " 'trì',\n",
       " 'ECMO',\n",
       " ',',\n",
       " 'thở',\n",
       " 'máy',\n",
       " ',',\n",
       " 'hiện',\n",
       " 'xơ',\n",
       " 'phổi',\n",
       " 'rất',\n",
       " 'nhiều',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspert.utils import read_json\n",
    "test_data = read_json(\"data/processed_data/phoner_covid19/test.json\")\n",
    "test_data[1]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
